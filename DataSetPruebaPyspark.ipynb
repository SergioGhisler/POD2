{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"/Users/sergio/Desktop/Trabajo proyecto2/allData.csv\")\n",
    "\n",
    "datosDF = sqlContext.createDataFrame(datos)\n",
    "datosDF=datosDF.drop(\"Unnamed: 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0 Aire_acondicionado Altura_sótano  Ancho_de_calle  \\\n",
      "1457        1457                  Y            TA            75.0   \n",
      "\n",
      "      Año_de_construcción  Año_de_reforma    Base  Baños_a_medias_por_encima_  \\\n",
      "1457                 1965            1965  CBlock                           1   \n",
      "\n",
      "      Baños_a_medias_sótano  Baños_enteros_por_encima      ...       \\\n",
      "1457                    0.0                         1      ...        \n",
      "\n",
      "      Tipo_de_calefacción Tipo_de_garaje Tipo_de_venta Tipo_de_vivienda  \\\n",
      "1457                 GasA         Attchd            WD             1Fam   \n",
      "\n",
      "      Total_pies_cuadr_sótano Valla  Valor_otras_caract Vecindario Zona  \\\n",
      "1457                   1256.0  None                   0    Edwards   RL   \n",
      "\n",
      "     PrecioDeVenta  \n",
      "1457        147500  \n",
      "\n",
      "[1 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "print(datos.loc[[1457]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= datosDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
    "    \n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols ]\n",
    "    \n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    \n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    \n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    \n",
    "    return data.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = catcols = ['Aire_acondicionado','Altura_sótano','Base','Calidad_chimeneas','Calidad_cocina','Calidad_garaje','Calidad_materiales_exterior','Calidad_y_condición_de_la_calefacción','Calidad_y_condición_piscina','Callejón','Clasificación_sótano_zona1','Clasificación_sótano_zona2','Condición_1','Condición_2','Condición_garaje','Condición_materiales_exterior','Condición_sótano','Condición_venta','Estilo_tejado','Funcionalidad_casa','Geografía_del_terreno','Localización','Material_tejado','Otras_características','Pavimento','Pavimento_entrada_coche','Pendiente','Plantas','Primer_revestimiento_exterior','Puertecita_sótano','Relieve','Segundo_revestimiento_exterior','Sistema_eléctrico','Situación_garaje','Tipo_de_bloque','Tipo_de_calefacción','Tipo_de_garaje','Tipo_de_venta','Tipo_de_vivienda','Valla','Vecindario','Zona']\n",
    "\n",
    "\n",
    "num_cols = ['Ancho_de_calle','Año_de_construcción','Año_de_reforma','Baños_a_medias_por_encima_','Baños_a_medias_sótano','Baños_enteros_por_encima','Baños_enteros_sótano','Calidad_materiales','Calidad_vivienda','Cantidad_bloques_pies_cuadrados','Chimeneas','Clasificación','Coches_garaje','Cocina_por_encima','Dormitorios_por_encima','Pes_cuadr_baja_calidad','Pes_cuadr_totales_(sin_garaje)','Pies_cuadr_cerramiento','Pies_cuadr_cubierta_madera','Pies_cuadr_piscina','Pies_cuadr_poche_abierto','Pies_cuadr_porcha_cerrado','Pies_cuadr_segundo_piso','Pies_cuadr_sótano_sin_acabar','Pies_cuadr_z1_sótano','Pies_cuadr_z2_sótano','Pies_cuadrados_terreno','Precio_mensual','PrecioAnual','ScreenPorch','Total_pies_cuadr_sótano','Valor_otras_caract']\n",
    "labelCol = 'PrecioDeVenta'\n",
    "\n",
    "data = get_dummy(datosDF,catcols,num_cols,labelCol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(253,[0,2,5,10,16...|208500|\n",
      "|(253,[0,2,6,12,15...|181500|\n",
      "|(253,[0,2,5,12,16...|223500|\n",
      "|(253,[0,1,7,11,16...|140000|\n",
      "|(253,[0,2,5,12,16...|250000|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+\n",
      "|            features| label|indexedLabel|\n",
      "+--------------------+------+------------+\n",
      "|(253,[0,2,5,10,16...|208500|       329.0|\n",
      "|(253,[0,2,6,12,15...|181500|       286.0|\n",
      "|(253,[0,2,5,12,16...|223500|       212.0|\n",
      "|(253,[0,1,7,11,16...|140000|         1.0|\n",
      "|(253,[0,2,5,12,16...|250000|        23.0|\n",
      "+--------------------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='label', outputCol='indexedLabel').fit(data)\n",
    "labelIndexer.transform(data).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|            features| label|     indexedFeatures|\n",
      "+--------------------+------+--------------------+\n",
      "|(253,[0,2,5,10,16...|208500|(253,[0,2,5,10,16...|\n",
      "|(253,[0,2,6,12,15...|181500|(253,[0,2,6,12,15...|\n",
      "|(253,[0,2,5,12,16...|223500|(253,[0,2,5,12,16...|\n",
      "|(253,[0,1,7,11,16...|140000|(253,[0,1,7,11,16...|\n",
      "|(253,[0,2,5,12,16...|250000|(253,[0,2,5,12,16...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "import pyspark.ml.classification as cl\n",
    "\n",
    "logistic = cl.LogisticRegression(\n",
    "    maxIter=10,\n",
    "    regParam=0.01,\n",
    "    labelCol='indexedLabel')\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                              outputCol=\"indexedFeatures\", \\\n",
    "                              maxCategories=4).fit(data)\n",
    "featureIndexer.transform(data).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"Id\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.createDataFrame(data.head(1458), data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = train.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='indexedFeatures', labelCol='indexedLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"SalePrice\",\n",
    "                              labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model. This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=prediction.select(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|SalePrice|\n",
      "+---------+\n",
      "|    87000|\n",
      "|   110000|\n",
      "|   172400|\n",
      "|   244600|\n",
      "|   120500|\n",
      "|   185900|\n",
      "|   143000|\n",
      "|    67000|\n",
      "|   148000|\n",
      "|   154000|\n",
      "|   119000|\n",
      "|   110000|\n",
      "|   325624|\n",
      "|   154000|\n",
      "|   143000|\n",
      "|   130500|\n",
      "|   154000|\n",
      "|   140000|\n",
      "|   178900|\n",
      "|   134500|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.toPandas().to_csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline3 = Pipeline(stages=[labelIndexer, featureIndexer, rf,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model3 = pipeline3.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = model3.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted3=predictions3.select(\"Id\",\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| Id|SalePrice|\n",
      "+---+---------+\n",
      "|  0|   140000|\n",
      "|  0|   140000|\n",
      "|  0|   140000|\n",
      "|  0|   220000|\n",
      "|  0|   140000|\n",
      "|  0|   140000|\n",
      "|  0|   140000|\n",
      "|  0|   125000|\n",
      "|  0|   140000|\n",
      "|  0|   145000|\n",
      "|  0|   140000|\n",
      "|  0|   140000|\n",
      "|  0|   320000|\n",
      "|  0|   175000|\n",
      "|  0|   133000|\n",
      "|  0|   130000|\n",
      "|  0|   139400|\n",
      "|  0|   140000|\n",
      "|  0|   263435|\n",
      "|  0|   140000|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted3.toPandas().to_csv('mycsv3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid= tune.ParamGridBuilder().addGrid(logistic.maxIter,[2,10,50]).addGrid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary.accuracy\n",
    "\n",
    "str(trainingSummary.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "str(trainingSummary.weightedPrecision)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dTree = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"SalePrice\",\n",
    "                               labels=labelIndexer.labels)\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dTree,labelConverter])\n",
    "# Train model.  This also runs the indexers.\n",
    "model2 = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = model2.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2=prediction2.select(\"Id\",\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2.toPandas().to_csv('mycsv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = NaiveBayes.train(trainingData, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import Row\n",
    "categoryIndexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=\"words\", outputCol=\"features\", numFeatures=10000)\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "categoryConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predCategory\", labels=categoryIndexer.labels)\n",
    "pipeline = Pipeline(stages=[categoryIndexer, tokenizer, hashingTF, nb, categoryConverter])\n",
    "\n",
    "model3 = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model1.transform(testDF)\n",
    "# Select example rows to display. \n",
    "predictions.select(\"features\",\"label\",\"predictedLabel\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = model2.stages[-2]\n",
    "trainingSummary2 = lrModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions) \n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "rfModel = model.stages[-2] \n",
    "print(rfModel) # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
