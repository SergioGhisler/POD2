{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.types as typ\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = pd.read_csv(\"allData.csv\")\n",
    "\n",
    "datosDF = sqlContext.createDataFrame(datos)\n",
    "datosDF=datosDF.drop(\"Unnamed: 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0 Aire_acondicionado Altura_sótano  Ancho_de_calle  \\\n",
      "1457        1457                  Y            TA            75.0   \n",
      "\n",
      "      Año_de_construcción  Año_de_reforma    Base  Baños_a_medias_por_encima_  \\\n",
      "1457                 1965            1965  CBlock                           1   \n",
      "\n",
      "      Baños_a_medias_sótano  Baños_enteros_por_encima      ...       \\\n",
      "1457                    0.0                         1      ...        \n",
      "\n",
      "      Tipo_de_calefacción Tipo_de_garaje Tipo_de_venta Tipo_de_vivienda  \\\n",
      "1457                 GasA         Attchd            WD             1Fam   \n",
      "\n",
      "      Total_pies_cuadr_sótano Valla  Valor_otras_caract Vecindario Zona  \\\n",
      "1457                   1256.0  None                   0    Edwards   RL   \n",
      "\n",
      "     PrecioDeVenta  \n",
      "1457        147500  \n",
      "\n",
      "[1 rows x 76 columns]\n"
     ]
    }
   ],
   "source": [
    "print(datos.loc[[1457]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= datosDF.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummy(df,categoricalCols,continuousCols,labelCol):\n",
    "    \n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "    from pyspark.sql.functions import col\n",
    "    \n",
    "    indexers = [ StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c)) for c in categoricalCols ]\n",
    "    \n",
    "    # default setting: dropLast=True\n",
    "    encoders = [ OneHotEncoder(inputCol=indexer.getOutputCol(), outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) for indexer in indexers ]\n",
    "    \n",
    "    assembler = VectorAssembler(inputCols=[encoder.getOutputCol() for encoder in encoders] + continuousCols, outputCol=\"features\")\n",
    "    \n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    \n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    \n",
    "    data = data.withColumn('label',col(labelCol))\n",
    "\n",
    "    \n",
    "    return data.select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "catcols = catcols = ['Aire_acondicionado','Altura_sótano','Base','Calidad_chimeneas','Calidad_cocina','Calidad_garaje','Calidad_materiales_exterior','Calidad_y_condición_de_la_calefacción','Calidad_y_condición_piscina','Callejón','Clasificación_sótano_zona1','Clasificación_sótano_zona2','Condición_1','Condición_2','Condición_garaje','Condición_materiales_exterior','Condición_sótano','Condición_venta','Estilo_tejado','Funcionalidad_casa','Geografía_del_terreno','Localización','Material_tejado','Otras_características','Pavimento','Pavimento_entrada_coche','Pendiente','Plantas','Primer_revestimiento_exterior','Puertecita_sótano','Relieve','Segundo_revestimiento_exterior','Sistema_eléctrico','Situación_garaje','Tipo_de_bloque','Tipo_de_calefacción','Tipo_de_garaje','Tipo_de_venta','Tipo_de_vivienda','Valla','Vecindario','Zona']\n",
    "\n",
    "\n",
    "num_cols = ['Ancho_de_calle','Año_de_construcción','Año_de_reforma','Baños_a_medias_por_encima_','Baños_a_medias_sótano','Baños_enteros_por_encima','Baños_enteros_sótano','Calidad_materiales','Calidad_vivienda','Cantidad_bloques_pies_cuadrados','Chimeneas','Clasificación','Coches_garaje','Cocina_por_encima','Dormitorios_por_encima','Pes_cuadr_baja_calidad','Pes_cuadr_totales_(sin_garaje)','Pies_cuadr_cerramiento','Pies_cuadr_cubierta_madera','Pies_cuadr_piscina','Pies_cuadr_poche_abierto','Pies_cuadr_porcha_cerrado','Pies_cuadr_segundo_piso','Pies_cuadr_sótano_sin_acabar','Pies_cuadr_z1_sótano','Pies_cuadr_z2_sótano','Pies_cuadrados_terreno','Precio_mensual','PrecioAnual','ScreenPorch','Total_pies_cuadr_sótano','Valor_otras_caract']\n",
    "labelCol = 'PrecioDeVenta'\n",
    "\n",
    "data = get_dummy(datosDF,catcols,num_cols,labelCol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(253,[0,2,5,10,16...|208500|\n",
      "|(253,[0,2,6,12,15...|181500|\n",
      "|(253,[0,2,5,12,16...|223500|\n",
      "|(253,[0,1,7,11,16...|140000|\n",
      "|(253,[0,2,5,12,16...|250000|\n",
      "+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------+\n",
      "|            features| label|indexedLabel|\n",
      "+--------------------+------+------------+\n",
      "|(253,[0,2,5,10,16...|208500|       329.0|\n",
      "|(253,[0,2,6,12,15...|181500|       286.0|\n",
      "|(253,[0,2,5,12,16...|223500|       212.0|\n",
      "|(253,[0,1,7,11,16...|140000|         1.0|\n",
      "|(253,[0,2,5,12,16...|250000|        23.0|\n",
      "+--------------------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Index labels, adding metadata to the label column\n",
    "labelIndexer = StringIndexer(inputCol='label', outputCol='indexedLabel').fit(data)\n",
    "labelIndexer.transform(data).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+--------------------+\n",
      "|            features| label|     indexedFeatures|\n",
      "+--------------------+------+--------------------+\n",
      "|(253,[0,2,5,10,16...|208500|(253,[0,2,5,10,16...|\n",
      "|(253,[0,2,6,12,15...|181500|(253,[0,2,6,12,15...|\n",
      "|(253,[0,2,5,12,16...|223500|(253,[0,2,5,12,16...|\n",
      "|(253,[0,1,7,11,16...|140000|(253,[0,1,7,11,16...|\n",
      "|(253,[0,2,5,12,16...|250000|(253,[0,2,5,12,16...|\n",
      "+--------------------+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorIndexer\n",
    "import pyspark.ml.classification as cl\n",
    "\n",
    "logistic = cl.LogisticRegression(\n",
    "    maxIter=10,\n",
    "    regParam=0.01,\n",
    "    labelCol='indexedLabel')\n",
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                              outputCol=\"indexedFeatures\", \\\n",
    "                              maxCategories=4).fit(data)\n",
    "featureIndexer.transform(data).show(5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"Id\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = sqlContext.createDataFrame(data.head(1458), data.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=data.subtract(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = train.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='indexedFeatures', labelCol='indexedLabel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logr.getRegParam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"SalePrice\",\n",
    "                              labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model. This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrModel = model.stages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingSummary = lrModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#trainingSummary4.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=prediction.select(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|SalePrice|\n",
      "+---------+\n",
      "|    87000|\n",
      "|   130000|\n",
      "|   210000|\n",
      "|   147000|\n",
      "|   125500|\n",
      "|   185900|\n",
      "|   143000|\n",
      "|    67000|\n",
      "|   128500|\n",
      "|   137500|\n",
      "|   110000|\n",
      "|    55993|\n",
      "|   446261|\n",
      "|   301000|\n",
      "|   133000|\n",
      "|   159000|\n",
      "|   154000|\n",
      "|    80000|\n",
      "|   167500|\n",
      "|   139400|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.toPandas().to_csv('mycsv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "grid = tune.ParamGridBuilder().addGrid(logistic.maxIter, [100,150,200]).addGrid(logr.regParam, [0.01, 0.05, 0.3]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.evaluation as ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = tune.CrossValidator(estimator=logistic, estimatorParamMaps=grid, evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer = model.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer = data_transformer.drop('prediction')\n",
    "data_transformer = data_transformer.drop('rawPrediction')\n",
    "data_transformer = data_transformer.drop('probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvModel = cv.fit(data_transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print ('Best Param (MaxIter): ', cvModel.bestModel._java_obj.getMaxIter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Best Param (Max): ', cvModel.bestModel._java_obj.getRegParam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "logr2 = LogisticRegression(featuresCol='indexedFeatures', labelCol='indexedLabel',maxIter=50,regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline4 = Pipeline(stages=[labelIndexer, featureIndexer, logr2,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = pipeline4.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainingSummary2 = lrModel2.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction4 = model4.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted4=prediction.select(\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|SalePrice|\n",
      "+---------+\n",
      "|    87000|\n",
      "|   130000|\n",
      "|   210000|\n",
      "|   147000|\n",
      "|   125500|\n",
      "|   185900|\n",
      "|   143000|\n",
      "|    67000|\n",
      "|   128500|\n",
      "|   137500|\n",
      "|   110000|\n",
      "|    55993|\n",
      "|   446261|\n",
      "|   301000|\n",
      "|   133000|\n",
      "|   159000|\n",
      "|   154000|\n",
      "|    80000|\n",
      "|   167500|\n",
      "|   139400|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted4.toPandas().to_csv('mycsv4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline3 = Pipeline(stages=[labelIndexer, featureIndexer, rf,labelConverter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model3 = pipeline3.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = model3.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted3=predictions3.select(\"Id\",\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| Id|SalePrice|\n",
      "+---+---------+\n",
      "|  0|   125000|\n",
      "|  0|   155000|\n",
      "|  0|   140000|\n",
      "|  0|   230000|\n",
      "|  0|   124000|\n",
      "|  0|   230000|\n",
      "|  0|   208900|\n",
      "|  0|    87000|\n",
      "|  0|   135000|\n",
      "|  0|   140000|\n",
      "|  0|   135000|\n",
      "|  0|   125000|\n",
      "|  0|   230000|\n",
      "|  0|   155000|\n",
      "|  0|   165500|\n",
      "|  0|   144000|\n",
      "|  0|   155000|\n",
      "|  0|   135000|\n",
      "|  0|   250000|\n",
      "|  0|   135000|\n",
      "+---+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted3.toPandas().to_csv('mycsv3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dTree = DecisionTreeClassifier(labelCol='indexedLabel', featuresCol='indexedFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"SalePrice\",\n",
    "                               labels=labelIndexer.labels)\n",
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dTree,labelConverter])\n",
    "# Train model.  This also runs the indexers.\n",
    "model2 = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = model2.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2=prediction2.select(\"Id\",\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2.toPandas().to_csv('mycsv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import ChiSqSelector\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ChiSqSelector(numTopFeatures=10, featuresCol=\"features\",\n",
    "                             outputCol=\"selectedFeatures\", labelCol=\"label\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChiSqSelector output with top 10 features selected\n"
     ]
    }
   ],
   "source": [
    "result = selector.fit(train)\n",
    "\n",
    "print(\"ChiSqSelector output with top %d features selected\" % selector.getNumTopFeatures())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = result.selectedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 17, 23, 24, 25, 61, 76, 79, 178, 198]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importantFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(253,[0,2,5,10,16...|\n",
      "|(253,[0,2,6,12,15...|\n",
      "|(253,[0,2,5,12,16...|\n",
      "|(253,[0,1,7,11,16...|\n",
      "|(253,[0,2,5,12,16...|\n",
      "|(253,[0,2,10,15,1...|\n",
      "|(253,[0,3,5,11,16...|\n",
      "|(253,[0,2,6,12,15...|\n",
      "|(253,[0,1,7,12,15...|\n",
      "|(253,[0,1,7,12,15...|\n",
      "|(253,[0,1,6,10,15...|\n",
      "|(253,[0,3,5,11,17...|\n",
      "|(253,[0,1,6,10,15...|\n",
      "|(253,[0,2,5,11,16...|\n",
      "|(253,[0,1,6,13,15...|\n",
      "|(253,[0,1,7,10,15...|\n",
      "|(253,[0,1,6,12,15...|\n",
      "|(253,[0,8,10,15,1...|\n",
      "|(253,[0,1,5,10,16...|\n",
      "|(253,[0,1,6,10,15...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select('features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-8484.7605, 1902.1165])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pca_features\")\n",
    "model = pca.fit(train)\n",
    "model.transform(train).collect()[0].pca_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method JavaMLReadable.read of <class 'pyspark.ml.feature.PCAModel'>>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
